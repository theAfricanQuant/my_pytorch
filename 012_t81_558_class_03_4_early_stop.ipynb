{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZXh7dogJlHH"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_03_4_early_stop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKH1QxMuJlHK"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "\n",
    "**Module 3: Introduction to PyTorch**\n",
    "\n",
    "- Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "- For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwkbs9-gJlHL"
   },
   "source": [
    "# Module 3 Material\n",
    "\n",
    "- Part 3.1: Deep Learning and Neural Network Introduction [[Video]](https://www.youtube.com/watch?v=d-rU5IuFqLs&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_03_1_neural_net.ipynb)\n",
    "- Part 3.2: Introduction to PyTorch [[Video]](https://www.youtube.com/watch?v=Pf-rrhMolm0&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_03_2_pytorch.ipynb)\n",
    "- Part 3.3: Encoding a Feature Vector for PyTorch Deep Learning [[Video]](https://www.youtube.com/watch?v=7SGPm2tIT58&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_03_3_feature_encode.ipynb)\n",
    "- **Part 3.4: Early Stopping and Network Persistence** [[Video]](https://www.youtube.com/watch?v=lS0vvIWiahU&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_03_4_early_stop.ipynb)\n",
    "- Part 3.5: Sequences vs Classes in PyTorch [[Video]](https://www.youtube.com/watch?v=NOu8jMZx3LY&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_03_5_pytorch_class_sequence.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovYF1H1ZJlHL"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed. We also initialize the PyTorch device to either GPU/MPS (if available) or CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4wO3BiMJlHM",
    "outputId": "4eca8b22-802a-47c9-eee3-365fad896474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.0\n",
      "CUDA is available on this device.\n",
      "Device Name: NVIDIA GeForce RTX 2060\n",
      "CUDA Version: 12.1\n",
      "Number of CUDA Devices: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "def check_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is available on this device.\")\n",
    "        print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"Number of CUDA Devices: {torch.cuda.device_count()}\")\n",
    "    else:\n",
    "        print(\"CUDA is not available on this device. Using CPU.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_cuda()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will:\n",
    "\n",
    "* Check if CUDA is available using `torch.cuda.is_available()`.\n",
    "* If CUDA is available, it will print the name of the CUDA device, the CUDA version, and the number of CUDA devices available.\n",
    "* If CUDA is not available, it will inform you that it's defaulting to the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.4: Early Stopping and Network Persistence\n",
    "\n",
    "In this comprehensive module, we are getting into the weeds of two fundamental and critically important aspects of training neural networks, which are the concept of early stopping and the mechanisms for saving and loading PyTorch network models. The idea behind early stopping is deceptively simple yet profoundly impactful in the world of machine learning and neural network training. This technique is all about vigilantly monitoring the loss incurred during the validation phase of your training process. By keeping a close eye on this, we can astutely identify the opportune moment to cease further training, thereby efficiently circumventing the common pitfall of overfitting and ensuring the model's performance is optimized. \n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "The concept of early stopping emerges as a pragmatic solution to a common quandary faced by many in the field: determining the ideal number of epochs for training a neural network. The peril of overtraining looms large; if a neural network is subjected to an excessive number of training epochs, it risks becoming overly specialized to the training data. This phenomenon, known as overfitting, manifests when the neural network, rather than learning to generalize from the input data, begins to memorize it. The visible symptom of this issue is a neural network that performs admirably on the training data, boasting high accuracy, yet falters and fails to replicate this success on new, unseen data. \n",
    "\n",
    "Moreover, this module is not just limited to exploring the nuances of early stopping. We also venture into the critical domain of saving and loading PyTorch networks. This aspect of the training process is indispensable as it grants us the ability to preserve the state of our carefully trained models. Once saved, these models can be conveniently reloaded for future use, whether for making predictions, further training, or analysis. This ability is particularly valuable as it offers a means to efficiently manage and utilize the trained networks we've invested substantial time and resources in. Through a thorough understanding and application of these techniques, you will be better equipped to not only enhance your models’ performance but also manage your neural network projects with greater efficacy and finesse.\n",
    "\n",
    "\n",
    "It can be difficult to determine how many epochs to cycle through to train a neural network. Overfitting will occur if you train the neural network for too many epochs, and the neural network will not perform well on new data, despite attaining a good accuracy on the training set. Overfitting occurs when a neural network is trained to the point that it begins to memorize rather than generalize, as demonstrated in Figure 3.\n",
    "\n",
    "**Figure 3.OVER: Training vs. Validation Error for Overfitting**\n",
    "![Training vs. Validation Error for Overfitting](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_3_training_val.png \"Training vs. Validation Error for Overfitting\")\n",
    "\n",
    "It is important to segment the original dataset into several datasets:\n",
    "\n",
    "- **Training Set**\n",
    "- **Validation Set**\n",
    "- **Holdout Set**\n",
    "\n",
    "You can construct these sets in several different ways. The following programs demonstrate some of these.\n",
    "\n",
    "The first method is a training and validation set. We use the training data to train the neural network until the validation set no longer improves. This attempts to stop at a near-optimal training point. This method will only give accurate \"out of sample\" predictions for the validation set; this is usually 20% of the data. The predictions for the training data will be overly optimistic, as these were the data that we used to train the neural network. Figure 3.VAL demonstrates how we divide the dataset.\n",
    "\n",
    "**Figure 3.VAL: Training with a Validation Set**\n",
    "![Training with a Validation Set](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_1_train_val.png \"Training with a Validation Set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfQxxVVs--7K"
   },
   "source": [
    "Because PyTorch does not include a built-in early stopping function, we must define one of our own. We will use the following **EarlyStopping** class throughout this course.\n",
    "\n",
    "We can provide several parameters to the **EarlyStopping** object:\n",
    "\n",
    "- **min_delta** This value should be kept small; it specifies the minimum change that should be considered an improvement. Setting it even smaller will not likely have a great deal of impact.\n",
    "- **patience** How long should the training wait for the validation error to improve?\n",
    "- **restore_best_weights** You should usually set this to true, as it restores the weights to the values they were at when the validation set is the highest.\n",
    "\n",
    "We will now see an example of this class in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    EarlyStopping utility for PyTorch models.\n",
    "\n",
    "    This class implements an early stopping mechanism to terminate training \n",
    "    when validation loss stops improving, helping prevent overfitting and saving \n",
    "    computational resources.\n",
    "\n",
    "    Attributes:\n",
    "        patience (int): Number of epochs to wait for improvement before stopping.\n",
    "        min_delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        restore_best_weights (bool): If True, restores model weights from the epoch with the lowest loss.\n",
    "\n",
    "    Methods:\n",
    "        __call__(model, val_loss): Call method to perform early stopping check.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_model = None\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.status = \"\"\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        \"\"\"\n",
    "        Perform early stopping check.\n",
    "\n",
    "        Args:\n",
    "            model: PyTorch model being trained.\n",
    "            val_loss (float): Current validation loss.\n",
    "\n",
    "        Returns:\n",
    "            True if early stopping is triggered, False otherwise.\n",
    "        \"\"\"\n",
    "\n",
    "        # First validation loss received; set as best and copy model\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Improvement found; update best loss and model, reset counter\n",
    "        elif self.best_loss - val_loss >= self.min_delta:\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
    "\n",
    "        # No improvement; increment counter and check for early stopping\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.status = f\"No improvement in the last {self.counter} epochs\"\n",
    "            if self.counter >= self.patience:\n",
    "                self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_model)\n",
    "                return True\n",
    "\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_model = None\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.status = \"\"\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "        elif self.best_loss - val_loss >= self.min_delta:\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.status = f\"No improvement in the last {self.counter} epochs\"\n",
    "            if self.counter >= self.patience:\n",
    "                self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_model)\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class: `EarlyStopping`\n",
    "The `EarlyStopping` class is designed to monitor the training process and stop it appropriately.\n",
    "\n",
    "#### Initialization (`__init__`):\n",
    "- `patience`: The number of epochs to wait for an improvement in the validation loss before stopping the training. Default is 5.\n",
    "- `min_delta`: The minimum change in the validation loss to qualify as an improvement. This helps in ignoring very small changes. Default is 0.\n",
    "- `restore_best_weights`: If `True`, when early stopping is triggered, the model's weights are reverted to the state where the validation loss was lowest.\n",
    "- `best_model`: To store the model's state dict (weights) at its best performance.\n",
    "- `best_loss`: To keep track of the best (lowest) validation loss observed.\n",
    "- `counter`: Counts how many epochs have passed without improvement in validation loss.\n",
    "- `status`: A string to store messages about the early stopping status.\n",
    "\n",
    "#### The Call Method (`__call__`):\n",
    "This method is executed each time an `EarlyStopping` object is called with parameters.\n",
    "\n",
    "- Parameters:\n",
    "    - `model`: The neural network model being trained.\n",
    "    - `val_loss`: The current epoch's validation loss.\n",
    "- Process:\n",
    "    - If `best_loss` is `None` (first call), it initializes `best_loss` with the current `val_loss` and copies the model's weights.\n",
    "    - If the current `val_loss` shows an improvement greater than or equal to `min_delta` compared to `best_loss`, update `best_model` and `best_loss` with the current model's state and `val_loss`, respectively. Reset `counter` to 0.\n",
    "    - If there's no improvement, increment the `counter`. If `counter` reaches `patience`, it implies no improvement in validation loss for a specified number of epochs, thus triggering early stopping.\n",
    "        - If `restore_best_weights` is `True`, the model's weights are reverted to the best observed state.\n",
    "        - Returns `True`, indicating early stopping is triggered.\n",
    "- If early stopping is not triggered, the method returns `False`.\n",
    "\n",
    "### Usage\n",
    "The `EarlyStopping` class is used during the training loop of a neural network. After each epoch, the training script would call an instance of `EarlyStopping`, passing in the current model and the validation loss for that epoch. The early stopping mechanism will then decide whether training should continue or stop based on the evolution of the validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrsobz8ZJlHO"
   },
   "source": [
    "### Early Stopping with Classification\n",
    "\n",
    "We will now see an example of classification training with early stopping. We will train the neural network until the error no longer improves on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9EHQ1_zWSfZH",
    "outputId": "dde9b907-0d85-4ba7-f372-337999428eb0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, tloss: 0.6026307344436646, vloss: 0.536555, : 100%|████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 17.10it/s]\n",
      "Epoch: 2, tloss: 0.36586472392082214, vloss: 0.277725, Improvement found, counter reset to 0: 100%|█████████████████████████| 7/7 [00:00<00:00, 108.20it/s]\n",
      "Epoch: 3, tloss: 0.15603022277355194, vloss: 0.187535, Improvement found, counter reset to 0: 100%|█████████████████████████| 7/7 [00:00<00:00, 203.37it/s]\n",
      "Epoch: 4, tloss: 0.05794892832636833, vloss: 0.154333, Improvement found, counter reset to 0: 100%|█████████████████████████| 7/7 [00:00<00:00, 188.59it/s]\n",
      "Epoch: 5, tloss: 0.18528980016708374, vloss: 0.076723, Improvement found, counter reset to 0: 100%|█████████████████████████| 7/7 [00:00<00:00, 206.79it/s]\n",
      "Epoch: 6, tloss: 0.12420051544904709, vloss: 0.061499, Improvement found, counter reset to 0: 100%|█████████████████████████| 7/7 [00:00<00:00, 190.56it/s]\n",
      "Epoch: 7, tloss: 0.03340417146682739, vloss: 0.045322, Improvement found, counter reset to 0: 100%|█████████████████████████| 7/7 [00:00<00:00, 208.51it/s]\n",
      "Epoch: 8, tloss: 0.09452517330646515, vloss: 0.032975, Improvement found, counter reset to 0: 100%|█████████████████████████| 7/7 [00:00<00:00, 193.76it/s]\n",
      "Epoch: 9, tloss: 0.005208518821746111, vloss: 0.023963, Improvement found, counter reset to 0: 100%|████████████████████████| 7/7 [00:00<00:00, 187.84it/s]\n",
      "Epoch: 10, tloss: 0.06230873242020607, vloss: 0.015515, Improvement found, counter reset to 0: 100%|████████████████████████| 7/7 [00:00<00:00, 196.69it/s]\n",
      "Epoch: 11, tloss: 0.08908902853727341, vloss: 0.038120, No improvement in the last 1 epochs: 100%|██████████████████████████| 7/7 [00:00<00:00, 216.45it/s]\n",
      "Epoch: 12, tloss: 0.034965530037879944, vloss: 0.026789, No improvement in the last 2 epochs: 100%|█████████████████████████| 7/7 [00:00<00:00, 198.90it/s]\n",
      "Epoch: 13, tloss: 0.06976649910211563, vloss: 0.018425, No improvement in the last 3 epochs: 100%|██████████████████████████| 7/7 [00:00<00:00, 174.85it/s]\n",
      "Epoch: 14, tloss: 0.013938340358436108, vloss: 0.010584, Improvement found, counter reset to 0: 100%|███████████████████████| 7/7 [00:00<00:00, 198.16it/s]\n",
      "Epoch: 15, tloss: 0.03223254159092903, vloss: 0.008987, Improvement found, counter reset to 0: 100%|████████████████████████| 7/7 [00:00<00:00, 219.16it/s]\n",
      "Epoch: 16, tloss: 0.009036802686750889, vloss: 0.016889, No improvement in the last 1 epochs: 100%|█████████████████████████| 7/7 [00:00<00:00, 202.97it/s]\n",
      "Epoch: 17, tloss: 0.009504606015980244, vloss: 0.014088, No improvement in the last 2 epochs: 100%|█████████████████████████| 7/7 [00:00<00:00, 194.08it/s]\n",
      "Epoch: 18, tloss: 0.05779397115111351, vloss: 0.012317, No improvement in the last 3 epochs: 100%|██████████████████████████| 7/7 [00:00<00:00, 209.87it/s]\n",
      "Epoch: 19, tloss: 0.001863101962953806, vloss: 0.012097, No improvement in the last 4 epochs: 100%|█████████████████████████| 7/7 [00:00<00:00, 220.97it/s]\n",
      "Epoch: 20, tloss: 0.010492893867194653, vloss: 0.011086, Early stopping triggered after 5 epochs.: 100%|████████████████████| 7/7 [00:00<00:00, 173.65it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv(\n",
    "        \"https://data.heatonresearch.com/data/t81-558/iris.csv\", na_values=[\"NA\", \"?\"]\n",
    "    )\n",
    "\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    x = df[[\"sepal_l\", \"sepal_w\", \"petal_l\", \"petal_w\"]].values\n",
    "    y = le.fit_transform(df[\"species\"])\n",
    "    species = le.classes_\n",
    "\n",
    "    # Split into validation and training sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.25, random_state=42\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # Numpy to Torch Tensor\n",
    "    x_train = torch.tensor(x_train, device=device, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, device=device, dtype=torch.long)\n",
    "\n",
    "    x_test = torch.tensor(x_test, device=device, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, device=device, dtype=torch.long)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, species\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test, species = load_data()\n",
    "\n",
    "# Create datasets\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Create model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(x_train.shape[1], 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 25),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(25, len(species)),\n",
    "    nn.LogSoftmax(dim=1),\n",
    ")\n",
    "\n",
    "# model = torch.compile(model,backend=\"aot_eager\").to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()  # cross entropy loss\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "es = EarlyStopping()\n",
    "\n",
    "epoch = 0\n",
    "done = False\n",
    "while epoch < 1000 and not done:\n",
    "    epoch += 1\n",
    "    steps = list(enumerate(dataloader_train))\n",
    "    pbar = tqdm.tqdm(steps)\n",
    "    model.train()\n",
    "    for i, (x_batch, y_batch) in pbar:\n",
    "        x_batch = x_batch.to(device)  # Move your input batch to the device\n",
    "        y_batch = y_batch.to(device)  # Move your target batch to the device\n",
    "\n",
    "        y_batch_pred = model(x_batch.to(device))\n",
    "        loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "        if i == len(steps) - 1:\n",
    "            model.eval()\n",
    "            pred = model(x_test)\n",
    "            vloss = loss_fn(pred, y_test)\n",
    "            if es(model, vloss):\n",
    "                done = True\n",
    "            pbar.set_description(\n",
    "                f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, {es.status}\"\n",
    "            )\n",
    "        else:\n",
    "            pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJCDY-FcP41U",
    "outputId": "901b0a97-32cd-443e-d89c-4c47985c633b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.008987347595393658\n"
     ]
    }
   ],
   "source": [
    "pred = model(x_test)\n",
    "vloss = loss_fn(pred, y_test)\n",
    "print(f\"Loss = {vloss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATJhTzRjJlHQ"
   },
   "source": [
    "As you can see from above, we did not use the total number of requested epochs. The neural network training stopped once the validation set no longer improved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0iNHDxNJlHR",
    "outputId": "9c60711a-d28b-4360-92d8-2205fe0d7fcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = model(x_test)\n",
    "_, predict_classes = torch.max(pred, 1)\n",
    "correct = accuracy_score(y_test.cpu(), predict_classes.cpu())\n",
    "print(f\"Accuracy: {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nR03ea5QJlHS"
   },
   "source": [
    "### Early Stopping with Regression\n",
    "\n",
    "The following code demonstrates how we can apply early stopping to a regression problem. The technique is similar to the early stopping for classification code that we just saw.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pTuEcZE4JlHS",
    "outputId": "0dffc406-fb3b-41ad-bb34-e3f9f942e725"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, tloss: 261.34014892578125, vloss: 672.386780, EStop:[]: 100%|████████████████████████████████████████████████████| 19/19 [00:00<00:00, 89.46it/s]\n",
      "Epoch: 2, tloss: 189.85879516601562, vloss: 192.848831, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 144.34it/s]\n",
      "Epoch: 3, tloss: 206.1345672607422, vloss: 180.863358, EStop:[Improvement found, counter reset to 0]: 100%|███████████████| 19/19 [00:00<00:00, 147.51it/s]\n",
      "Epoch: 4, tloss: 189.10606384277344, vloss: 174.722336, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 147.88it/s]\n",
      "Epoch: 5, tloss: 200.2677459716797, vloss: 170.232391, EStop:[Improvement found, counter reset to 0]: 100%|███████████████| 19/19 [00:00<00:00, 147.68it/s]\n",
      "Epoch: 6, tloss: 180.11241149902344, vloss: 165.325562, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 154.35it/s]\n",
      "Epoch: 7, tloss: 122.46980285644531, vloss: 152.626953, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 165.32it/s]\n",
      "Epoch: 8, tloss: 142.1747283935547, vloss: 144.341232, EStop:[Improvement found, counter reset to 0]: 100%|███████████████| 19/19 [00:00<00:00, 166.68it/s]\n",
      "Epoch: 9, tloss: 67.58553314208984, vloss: 133.457550, EStop:[Improvement found, counter reset to 0]: 100%|███████████████| 19/19 [00:00<00:00, 186.83it/s]\n",
      "Epoch: 10, tloss: 75.73150634765625, vloss: 118.177559, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 162.02it/s]\n",
      "Epoch: 11, tloss: 92.48854064941406, vloss: 126.596375, EStop:[No improvement in the last 1 epochs]: 100%|████████████████| 19/19 [00:00<00:00, 184.05it/s]\n",
      "Epoch: 12, tloss: 73.30278778076172, vloss: 86.829865, EStop:[Improvement found, counter reset to 0]: 100%|███████████████| 19/19 [00:00<00:00, 153.94it/s]\n",
      "Epoch: 13, tloss: 100.01973724365234, vloss: 69.598755, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 158.38it/s]\n",
      "Epoch: 14, tloss: 62.98878860473633, vloss: 55.573479, EStop:[Improvement found, counter reset to 0]: 100%|███████████████| 19/19 [00:00<00:00, 148.66it/s]\n",
      "Epoch: 15, tloss: 42.859291076660156, vloss: 43.179432, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 140.31it/s]\n",
      "Epoch: 16, tloss: 40.42467498779297, vloss: 50.241421, EStop:[No improvement in the last 1 epochs]: 100%|█████████████████| 19/19 [00:00<00:00, 146.13it/s]\n",
      "Epoch: 17, tloss: 5.740864276885986, vloss: 54.786152, EStop:[No improvement in the last 2 epochs]: 100%|█████████████████| 19/19 [00:00<00:00, 135.02it/s]\n",
      "Epoch: 18, tloss: 37.3491096496582, vloss: 31.465488, EStop:[Improvement found, counter reset to 0]: 100%|████████████████| 19/19 [00:00<00:00, 142.73it/s]\n",
      "Epoch: 19, tloss: 47.07155990600586, vloss: 60.890282, EStop:[No improvement in the last 1 epochs]: 100%|█████████████████| 19/19 [00:00<00:00, 168.01it/s]\n",
      "Epoch: 20, tloss: 17.79499626159668, vloss: 50.140408, EStop:[No improvement in the last 2 epochs]: 100%|█████████████████| 19/19 [00:00<00:00, 169.41it/s]\n",
      "Epoch: 21, tloss: 26.974218368530273, vloss: 27.666313, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 177.71it/s]\n",
      "Epoch: 22, tloss: 32.735713958740234, vloss: 24.210747, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 128.75it/s]\n",
      "Epoch: 23, tloss: 26.935260772705078, vloss: 23.672148, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 130.87it/s]\n",
      "Epoch: 24, tloss: 44.15737533569336, vloss: 21.878447, EStop:[Improvement found, counter reset to 0]: 100%|███████████████| 19/19 [00:00<00:00, 112.83it/s]\n",
      "Epoch: 25, tloss: 34.703826904296875, vloss: 21.751738, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 143.56it/s]\n",
      "Epoch: 26, tloss: 18.243078231811523, vloss: 20.960989, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 154.77it/s]\n",
      "Epoch: 27, tloss: 17.406949996948242, vloss: 18.943769, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 148.27it/s]\n",
      "Epoch: 28, tloss: 13.00816822052002, vloss: 23.957792, EStop:[No improvement in the last 1 epochs]: 100%|█████████████████| 19/19 [00:00<00:00, 162.59it/s]\n",
      "Epoch: 29, tloss: 33.604454040527344, vloss: 16.431414, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 144.69it/s]\n",
      "Epoch: 30, tloss: 33.368492126464844, vloss: 16.989378, EStop:[No improvement in the last 1 epochs]: 100%|████████████████| 19/19 [00:00<00:00, 159.64it/s]\n",
      "Epoch: 31, tloss: 20.35410499572754, vloss: 23.444483, EStop:[No improvement in the last 2 epochs]: 100%|█████████████████| 19/19 [00:00<00:00, 172.56it/s]\n",
      "Epoch: 32, tloss: 7.345109462738037, vloss: 14.537473, EStop:[Improvement found, counter reset to 0]: 100%|███████████████| 19/19 [00:00<00:00, 166.82it/s]\n",
      "Epoch: 33, tloss: 15.93979549407959, vloss: 17.934063, EStop:[No improvement in the last 1 epochs]: 100%|█████████████████| 19/19 [00:00<00:00, 154.19it/s]\n",
      "Epoch: 34, tloss: 18.714248657226562, vloss: 13.295046, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 112.99it/s]\n",
      "Epoch: 35, tloss: 7.033111095428467, vloss: 12.081238, EStop:[Improvement found, counter reset to 0]: 100%|███████████████| 19/19 [00:00<00:00, 141.53it/s]\n",
      "Epoch: 36, tloss: 8.60721206665039, vloss: 11.788025, EStop:[Improvement found, counter reset to 0]: 100%|████████████████| 19/19 [00:00<00:00, 133.25it/s]\n",
      "Epoch: 37, tloss: 2.2952606678009033, vloss: 12.488453, EStop:[No improvement in the last 1 epochs]: 100%|████████████████| 19/19 [00:00<00:00, 162.72it/s]\n",
      "Epoch: 38, tloss: 13.84727954864502, vloss: 13.275508, EStop:[No improvement in the last 2 epochs]: 100%|█████████████████| 19/19 [00:00<00:00, 192.47it/s]\n",
      "Epoch: 39, tloss: 17.288183212280273, vloss: 11.650888, EStop:[Improvement found, counter reset to 0]: 100%|██████████████| 19/19 [00:00<00:00, 171.22it/s]\n",
      "Epoch: 40, tloss: 20.600814819335938, vloss: 19.881666, EStop:[No improvement in the last 1 epochs]: 100%|████████████████| 19/19 [00:00<00:00, 192.11it/s]\n",
      "Epoch: 41, tloss: 20.578567504882812, vloss: 16.989861, EStop:[No improvement in the last 2 epochs]: 100%|████████████████| 19/19 [00:00<00:00, 185.32it/s]\n",
      "Epoch: 42, tloss: 16.71962547302246, vloss: 10.719069, EStop:[Improvement found, counter reset to 0]: 100%|███████████████| 19/19 [00:00<00:00, 195.27it/s]\n",
      "Epoch: 43, tloss: 7.936643123626709, vloss: 9.597141, EStop:[Improvement found, counter reset to 0]: 100%|████████████████| 19/19 [00:00<00:00, 186.20it/s]\n",
      "Epoch: 44, tloss: 29.034513473510742, vloss: 9.041056, EStop:[Improvement found, counter reset to 0]: 100%|███████████████| 19/19 [00:00<00:00, 190.44it/s]\n",
      "Epoch: 45, tloss: 3.9655864238739014, vloss: 10.987778, EStop:[No improvement in the last 1 epochs]: 100%|████████████████| 19/19 [00:00<00:00, 191.00it/s]\n",
      "Epoch: 46, tloss: 7.568040370941162, vloss: 9.403249, EStop:[No improvement in the last 2 epochs]: 100%|██████████████████| 19/19 [00:00<00:00, 190.08it/s]\n",
      "Epoch: 47, tloss: 9.794283866882324, vloss: 8.315842, EStop:[Improvement found, counter reset to 0]: 100%|████████████████| 19/19 [00:00<00:00, 183.15it/s]\n",
      "Epoch: 48, tloss: 18.717222213745117, vloss: 8.309563, EStop:[Improvement found, counter reset to 0]: 100%|███████████████| 19/19 [00:00<00:00, 197.76it/s]\n",
      "Epoch: 49, tloss: 2.7942044734954834, vloss: 10.559594, EStop:[No improvement in the last 1 epochs]: 100%|████████████████| 19/19 [00:00<00:00, 216.15it/s]\n",
      "Epoch: 50, tloss: 2.9968879222869873, vloss: 8.354445, EStop:[No improvement in the last 2 epochs]: 100%|█████████████████| 19/19 [00:00<00:00, 215.24it/s]\n",
      "Epoch: 51, tloss: 7.3161749839782715, vloss: 8.509159, EStop:[No improvement in the last 3 epochs]: 100%|█████████████████| 19/19 [00:00<00:00, 194.06it/s]\n",
      "Epoch: 52, tloss: 3.444265842437744, vloss: 8.402481, EStop:[No improvement in the last 4 epochs]: 100%|██████████████████| 19/19 [00:00<00:00, 198.34it/s]\n",
      "Epoch: 53, tloss: 17.53005027770996, vloss: 17.295700, EStop:[Early stopping triggered after 5 epochs.]: 100%|████████████| 19/19 [00:00<00:00, 211.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Read the MPG dataset.\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", na_values=[\"NA\", \"?\"]\n",
    ")\n",
    "\n",
    "cars = df[\"name\"]\n",
    "\n",
    "# Handle missing value\n",
    "df[\"horsepower\"] = df[\"horsepower\"].fillna(df[\"horsepower\"].median())\n",
    "\n",
    "# Pandas to Numpy\n",
    "x = df[\n",
    "    [\n",
    "        \"cylinders\",\n",
    "        \"displacement\",\n",
    "        \"horsepower\",\n",
    "        \"weight\",\n",
    "        \"acceleration\",\n",
    "        \"year\",\n",
    "        \"origin\",\n",
    "    ]\n",
    "].values\n",
    "y = df[\"mpg\"].values  # regression\n",
    "\n",
    "# Split into validation and training sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Numpy to Torch Tensor\n",
    "x_train = torch.tensor(x_train, device=device, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, device=device, dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor(x_test, device=device, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# Create model\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(x_train.shape[1], 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 25),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(25, 1)\n",
    ")\n",
    "\n",
    "# model = torch.compile(model, backend=\"aot_eager\").to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function for regression\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "es = EarlyStopping()\n",
    "\n",
    "epoch = 0\n",
    "done = False\n",
    "while epoch < 1000 and not done:\n",
    "    epoch += 1\n",
    "    steps = list(enumerate(dataloader_train))\n",
    "    pbar = tqdm.tqdm(steps)\n",
    "    model.train()\n",
    "    for i, (x_batch, y_batch) in pbar:\n",
    "        y_batch_pred = model(x_batch).flatten()  #\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "        if i == len(steps) - 1:\n",
    "            model.eval()\n",
    "            pred = model(x_test).flatten()\n",
    "            vloss = loss_fn(pred, y_test)\n",
    "            if es(model, vloss):\n",
    "                done = True\n",
    "            pbar.set_description(\n",
    "                f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, EStop:[{es.status}]\"\n",
    "            )\n",
    "        else:\n",
    "            pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjvaHmp5JlHS"
   },
   "source": [
    "Finally, we evaluate the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bvqiX-AJlHS",
    "outputId": "b2cb2433-2bfd-4e28-ce4d-ba89d3b03d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 2.882631301879883\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "pred = model(x_test)\n",
    "score = torch.sqrt(torch.nn.functional.mse_loss(pred.flatten(), y_test))\n",
    "print(f\"Final score (RMSE): {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYi-h2LNXqoZ",
    "outputId": "14669b13-b693-44d4-de6b-7150e9629302"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([33.0000, 28.0000, 19.0000, 13.0000, 14.0000, 27.0000, 24.0000, 13.0000,\n",
       "        17.0000, 21.0000, 15.0000, 38.0000, 26.0000, 15.0000, 25.0000, 12.0000,\n",
       "        31.0000, 17.0000, 16.0000, 31.0000, 22.0000, 22.0000, 22.0000, 33.5000,\n",
       "        18.0000, 44.0000, 26.0000, 24.5000, 18.1000, 12.0000, 27.0000, 36.0000,\n",
       "        23.0000, 24.0000, 37.2000, 16.0000, 21.0000, 19.2000, 16.0000, 29.0000,\n",
       "        26.8000, 27.0000, 18.0000, 10.0000, 23.0000, 36.0000, 26.0000, 25.0000,\n",
       "        25.0000, 25.0000, 22.0000, 34.1000, 32.4000, 13.0000, 23.5000, 14.0000,\n",
       "        18.5000, 29.8000, 28.0000, 19.0000, 11.0000, 33.0000, 23.0000, 21.0000,\n",
       "        23.0000, 25.0000, 23.8000, 34.4000, 24.5000, 13.0000, 34.7000, 14.0000,\n",
       "        15.0000, 18.0000, 25.0000, 19.9000, 17.5000, 28.0000, 29.0000, 17.0000,\n",
       "        16.0000, 27.0000, 37.0000, 36.1000, 23.0000, 14.0000, 32.8000, 29.9000,\n",
       "        20.0000, 12.0000, 15.5000, 23.7000, 24.0000, 36.0000, 19.0000, 38.0000,\n",
       "        29.0000, 21.5000, 27.9000, 14.0000], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqUztPo3JlHT"
   },
   "source": [
    "## Saving and Loading a PyTorch Neural Network\n",
    "\n",
    "Complex neural networks will take a long time to fit/train. It is helpful to be able to save these neural networks so that you can reload them later. A reloaded neural network will not require retraining. PyTorch usually saves neural networks as [pickle](https://wiki.python.org/moin/UsingPickle) files. The following code trains a neural network to predict car MPG and saves the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PpXoGqn9SfZL",
    "outputId": "d443a679-7842-4ba1-96b0-846ceb51bcc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 743.8233642578125\n",
      "Epoch 100, loss: 13.698925018310547\n",
      "Epoch 200, loss: 11.317089080810547\n",
      "Epoch 300, loss: 8.084850311279297\n",
      "Epoch 400, loss: 6.356418132781982\n",
      "Epoch 500, loss: 4.847251892089844\n",
      "Epoch 600, loss: 5.083579063415527\n",
      "Epoch 700, loss: 5.602072715759277\n",
      "Epoch 800, loss: 2.9818522930145264\n",
      "Epoch 900, loss: 2.6347310543060303\n",
      "Before save score (RMSE): 2.0877859592437744\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read the MPG dataset.\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", na_values=[\"NA\", \"?\"]\n",
    ")\n",
    "\n",
    "# Handle missing value\n",
    "df[\"horsepower\"] = df[\"horsepower\"].fillna(df[\"horsepower\"].median())\n",
    "\n",
    "# Select features and target\n",
    "features = df[\n",
    "    [\n",
    "        \"cylinders\",\n",
    "        \"displacement\",\n",
    "        \"horsepower\",\n",
    "        \"weight\",\n",
    "        \"acceleration\",\n",
    "        \"year\",\n",
    "        \"origin\",\n",
    "    ]\n",
    "]\n",
    "target = df[\"mpg\"]\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert Numpy to PyTorch tensors\n",
    "features_tensor = torch.tensor(\n",
    "    scaled_features, device=device, dtype=torch.float32)\n",
    "target_tensor = torch.tensor(target.values, device=device, dtype=torch.float32)\n",
    "\n",
    "# Convert to TensorDataset\n",
    "dataset = TensorDataset(features_tensor, target_tensor)\n",
    "\n",
    "# Convert to DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "# Define the neural network using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(features_tensor.shape[1], 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 25),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(25, 1),\n",
    ").to(device)\n",
    "\n",
    "# Define the loss function for regression\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train for 1000 epochs.\n",
    "model.train()\n",
    "for epoch in range(1000):\n",
    "    for batch_features, batch_target in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch_features).flatten()\n",
    "        loss = loss_fn(out, batch_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Display status every 100 epochs.\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, loss: {loss.item()}\")\n",
    "\n",
    "model.eval()\n",
    "pred = model(features_tensor)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = torch.sqrt(torch.nn.functional.mse_loss(pred.flatten(), target_tensor))\n",
    "print(f\"Before save score (RMSE): {score}\")\n",
    "torch.save(model, \"mpg.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxJjG3JmSfZL"
   },
   "source": [
    "The code below sets up a neural network and reads the data (for predictions), but it does not clear the model directory or fit the neural network. The code loads the weights from the previous fit. Now we reload the network and perform another prediction. The RMSE should match the previous one exactly if we saved and reloaded the neural network correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jiYHZx4fSfZL",
    "outputId": "0104623c-c330-4dbf-e615-382913f1a786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before save score (RMSE): 2.0877859592437744\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error for loaded network.  RMSE is common for regression.\n",
    "model.eval()\n",
    "pred = model(features_tensor)\n",
    "score = torch.sqrt(torch.nn.functional.mse_loss(pred.flatten(), target_tensor))\n",
    "print(f\"Before save score (RMSE): {score}\")\n",
    "torch.save(model, \"mpg.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
